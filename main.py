import streamlit as st
import pandas as pd
import re
from datetime import datetime
from mapping_utils import (
    apply_all_name_replacements,
    replace_all_names_with_mapping,
    clean_mapping_headers,
    split_mapping_data,
)
from io import BytesIO

st.set_page_config("ğŸ“Š å¤šé¢„æµ‹æ•´åˆå·¥å…·", layout="wide")
st.title("ğŸ“Š å¤šé¢„æµ‹æ•´åˆä¸å“åæå–")

# ===== ä¸Šä¼ åŒº =====
forecast_files = st.file_uploader("ğŸ“ ä¸Šä¼ å¤šä¸ªé¢„æµ‹æ–‡ä»¶", type=["xlsx"], accept_multiple_files=True)
order_file = st.file_uploader("ğŸ“„ ä¸Šä¼ è®¢å•æ–‡ä»¶ï¼ˆå«â€œæ™¶åœ†å“åâ€ï¼‰", type=["xlsx"])
sales_file = st.file_uploader("ğŸ“„ ä¸Šä¼ å‡ºè´§æ–‡ä»¶ï¼ˆå«â€œå“åã€æ™¶åœ†ã€è§„æ ¼â€ï¼‰", type=["xlsx"])
mapping_file = st.file_uploader("ğŸ§­ ä¸Šä¼ æ–°æ—§æ–™å·æ˜ å°„è¡¨", type=["xlsx"])

# ===== é…ç½®å­—æ®µæ˜ å°„ï¼ˆç”¨äº apply_all_name_replacementsï¼‰ =====
FIELD_MAPPINGS = {
    "é¢„æµ‹": {"å“å": "å“å"},
    "è®¢å•": {"å“å": "æ™¶åœ†å“å"},
    "å‡ºè´§": {"å“å": "å“å"},
}

# ===== æŒ‰é’®è§¦å‘ä¸»æµç¨‹ =====
if st.button("ğŸš€ å¼€å§‹å¤„ç†") and forecast_files and order_file and sales_file and mapping_file:
    # 1ï¸âƒ£ è§£ææ–°æ—§æ–™å·æ˜ å°„
    mapping_raw = pd.read_excel(mapping_file)
    st.write(mapping_raw)
    mapping_df = clean_mapping_headers(mapping_raw)
    st.write(mapping_df)
    mapping_semi, mapping_new, mapping_sub = split_mapping_data(mapping_df)

    current_year = datetime.now().year
    all_parts = []

    # 2ï¸âƒ£ å¤„ç†æ¯ä¸ªé¢„æµ‹æ–‡ä»¶
    for uploaded_file in forecast_files:
        filename = uploaded_file.name
        # è¯»å–æœ€é•¿ sheet
        xls = pd.ExcelFile(uploaded_file)
        sheet_lengths = {sheet: pd.read_excel(xls, sheet).shape[0] for sheet in xls.sheet_names}
        longest_sheet = max(sheet_lengths, key=sheet_lengths.get)
        df_forecast = pd.read_excel(xls, sheet_name=longest_sheet, header=None)
    
        # æ£€æµ‹ header è¡Œï¼šå‰ä¸‰è¡Œä¸­æœ‰â€œäº§å“å‹å·â€è€…ä¸º header
        header_row = None
        for i in range(3):
            if any("äº§å“å‹å·" in str(cell) for cell in df_forecast.iloc[i]):
                header_row = i
                break
        if header_row is None:
            for i in range(3):
                if any(re.search(r"\d{1,2}æœˆé¢„æµ‹", str(cell)) for cell in df_forecast.iloc[i]):
                    header_row = i
                    break
    
        if header_row is not None:
            df_forecast.columns = df_forecast.iloc[header_row]
            df_forecast = df_forecast.iloc[header_row + 1:].reset_index(drop=True)
    
            # âœ… é‡å‘½åç¬¬2åˆ—ä¸ºâ€œå“åâ€ï¼ˆé˜²æ­¢å­—æ®µåå¼‚å¸¸ï¼‰
            if df_forecast.shape[1] >= 2:
                df_forecast.columns = list(df_forecast.columns)
                df_forecast.columns.values[1] = "å“å"
        else:
            st.warning(f"âš ï¸ æ— æ³•è¯†åˆ«é¢„æµ‹æ–‡ä»¶ `{filename}` çš„ headerï¼Œå·²è·³è¿‡")
            continue
    
        # ğŸ‘€ æ˜¾ç¤ºé¢„æµ‹æ•°æ®
        st.write(f"ğŸ“ è¯»å–åˆ°çš„é¢„æµ‹æ–‡ä»¶ `{filename}`ï¼š", df_forecast.head())
    
        df_forecast = df_forecast.rename(columns=lambda x: str(x).strip())
        if "å“å" not in df_forecast.columns:
            st.warning(f"âš ï¸ é¢„æµ‹æ–‡ä»¶ `{filename}` ç¼ºå°‘â€œå“åâ€åˆ—ï¼Œå·²è·³è¿‡")
            continue
    
        df_forecast = df_forecast[["å“å"]].copy()
        df_forecast["å“å"] = df_forecast["å“å"].astype(str).str.strip()
    
        # æ›¿æ¢æ–°æ—§æ–™å·
        df_forecast, _ = apply_all_name_replacements(
            df_forecast,
            mapping_new,
            mapping_sub,
            sheet_name="é¢„æµ‹",
            field_mappings=FIELD_MAPPINGS,
        )
        all_parts.append(df_forecast)


    # 3ï¸âƒ£ å¤„ç†è®¢å•æ–‡ä»¶ï¼ˆSheetï¼‰
    df_order = pd.read_excel(order_file, sheet_name="Sheet")
    st.write("ğŸ“„ è¯»å–åˆ°çš„è®¢å•æ•°æ®ï¼š", df_order.head())
    
    if "æ™¶åœ†å“å" not in df_order.columns:
        st.error("âŒ è®¢å•æ–‡ä»¶ä¸­ç¼ºå°‘â€œæ™¶åœ†å“åâ€å­—æ®µï¼Œè¯·æ£€æŸ¥ Sheet è¡¨æ ¼ã€‚")
        st.stop()
    
    df_order["æ™¶åœ†å“å"] = df_order["æ™¶åœ†å“å"].astype(str).str.strip()
    df_order, _ = apply_all_name_replacements(
        df_order, mapping_new, mapping_sub, "è®¢å•", FIELD_MAPPINGS
    )
    all_parts.append(df_order[["æ™¶åœ†å“å"]].rename(columns={"æ™¶åœ†å“å": "å“å"}))
    
    
    # 4ï¸âƒ£ å¤„ç†å‡ºè´§æ–‡ä»¶ï¼ˆåŸè¡¨ï¼‰
    df_sales = pd.read_excel(sales_file, sheet_name="åŸè¡¨")
    st.write("ğŸ“„ è¯»å–åˆ°çš„å‡ºè´§æ•°æ®ï¼š", df_sales.head())
    
    if "å“å" not in df_sales.columns:
        st.error("âŒ å‡ºè´§æ–‡ä»¶ä¸­ç¼ºå°‘â€œå“åâ€å­—æ®µï¼Œè¯·æ£€æŸ¥ åŸè¡¨ è¡¨æ ¼ã€‚")
        st.stop()
    
    df_sales["å“å"] = df_sales["å“å"].astype(str).str.strip()
    df_sales, _ = apply_all_name_replacements(
        df_sales, mapping_new, mapping_sub, "å‡ºè´§", FIELD_MAPPINGS
    )
    all_parts.append(df_sales[["å“å"]])

    # 5ï¸âƒ£ åˆå¹¶å»é‡å“åå¹¶è¿›è¡Œå†æ¬¡ç»Ÿä¸€æ›¿æ¢
    combined_names = pd.concat(all_parts, ignore_index=True)
    all_names = combined_names["å“å"].dropna().drop_duplicates().reset_index(drop=True)
    replaced_names = replace_all_names_with_mapping(all_names, mapping_new, mapping_sub)

    # 6ï¸âƒ£ æ„é€ æ€»è¡¨ï¼šæ™¶åœ† + è§„æ ¼ + å“åï¼Œä¼˜å…ˆä» mapping è¡¨ä¸­å–
    mapping_dict = mapping_new.set_index("æ–°å“å")[["æ–°æ™¶åœ†", "æ–°è§„æ ¼"]].copy()
    mapping_dict.columns = ["æ™¶åœ†", "è§„æ ¼"]
    
    df_final = pd.DataFrame({"å“å": replaced_names})
    df_final = df_final.merge(mapping_dict, how="left", left_on="å“å", right_index=True)
    
    # ğŸ§½ æ¸…ç†å±•ç¤ºç”¨ DataFrameï¼Œé˜²æ­¢ Arrow é”™è¯¯
    df_display = df_final.copy()
    df_display.columns = df_display.columns.map(str)
    for col in df_display.columns:
        df_display[col] = df_display[col].astype(str)
    
    # æ˜¾ç¤ºå½“å‰åˆæ­¥ç»“æœ
    st.write("ğŸ” æ›¿æ¢åçš„ä¸»å“åè¡¨ï¼ˆå«è§„æ ¼ä¸æ™¶åœ†ï¼‰å‰å‡ è¡Œï¼š", df_display.head())
    
    # ä»è®¢å•æˆ–å‡ºè´§ä¸­è¡¥å……ç¼ºå¤±è§„æ ¼/æ™¶åœ†
    missing_spec = df_final["è§„æ ¼"].isna()
    if missing_spec.any():
        # åˆå¹¶å‡ºè´§å’Œè®¢å•å­—æ®µï¼ˆå“åã€è§„æ ¼ã€æ™¶åœ†ï¼‰
        alt_spec = (
            pd.concat([df_order.rename(columns={"æ™¶åœ†å“å": "å“å"}), df_sales], ignore_index=True)
            .dropna(subset=["å“å"])
            .drop_duplicates(subset=["å“å"])  # ğŸ›¡ï¸ ç¡®ä¿å”¯ä¸€
            [["å“å", "è§„æ ¼", "æ™¶åœ†"]]
        )
    
        # ğŸ” æ–­è¨€åˆå¹¶å‰å”¯ä¸€æ€§
        assert alt_spec["å“å"].is_unique, "âŒ alt_spec ä¸­å“åä¸æ˜¯å”¯ä¸€çš„"
    
        # åˆå¹¶è¡¥è§„æ ¼
        df_final = df_final.merge(alt_spec, on="å“å", how="left", suffixes=("", "_alt"))
        df_final["è§„æ ¼"] = df_final["è§„æ ¼"].fillna(df_final["è§„æ ¼_alt"])
        df_final["æ™¶åœ†"] = df_final["æ™¶åœ†"].fillna(df_final["æ™¶åœ†_alt"])
        df_final = df_final.drop(columns=["è§„æ ¼_alt", "æ™¶åœ†_alt"])
    
    # âœ… æœ€ç»ˆç»“æœå±•ç¤º
    df_final = df_final[["æ™¶åœ†", "è§„æ ¼", "å“å"]]
    
    # å±•ç¤ºå‰å†æ¬¡æ¸…ç†ï¼Œç¡®ä¿å®‰å…¨æ˜¾ç¤º
    df_final_display = df_final.copy()
    df_final_display.columns = df_final_display.columns.map(str)
    for col in df_final_display.columns:
        df_final_display[col] = df_final_display[col].astype(str)
    
    st.success("âœ… æ€»å“åè¡¨ç”ŸæˆæˆåŠŸï¼")
    st.dataframe(df_final_display, use_container_width=True)
    
    # ğŸ“¥ ä¸‹è½½
    csv = df_final.to_csv(index=False, encoding="utf-8-sig")
    st.download_button("ğŸ“¥ ä¸‹è½½ç»“æœ CSV", csv, file_name="æ€»å“ååˆ—è¡¨.csv")
